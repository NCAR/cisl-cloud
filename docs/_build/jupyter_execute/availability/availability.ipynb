{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available Services and Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JupyterHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [On-premise](../how-to/k8sJH/k8sJH-intro)\n",
    "***STATUS:*** *In Development*\n",
    "\n",
    "**URL :** [https://jupyter.k8s.ucar.edu/](https://jupyter.k8s.ucar.edu/)\n",
    "\n",
    "\n",
    "An on-premise JupyterHub is up and running, but is still in development. Authentication against github is being worked on currently but there is no authentication presently. The user space that is spun up has access to a shared NFS volume that is read-only as well as GLADE collections and campaign directories also mounted as read-only. The user notebook that is deployed is based on a custom Docker image that the CCPP team maintains. Documentation on how this was setup and deployed can be found on the [how-to](../how-to/K8s/customize-docker) page in this documentation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS instance supported by 2i2c](../how-to/2i2cJH/2i2cJH-intro)\n",
    "***STATUS:*** *In Testing*\n",
    "\n",
    "**URL :** [https://ncar-cisl.2i2c.cloud/](https://ncar-cisl.2i2c.cloud/)\n",
    "\n",
    "The JupyterHub instance setup and managed by 2i2c that runs on AWS is up and ready to use. Access to this JupyterHub is controlled via a GitHub team, specifically the NCAR organizations [2i2c-cloud-users](https://github.com/orgs/NCAR/teams/2i2c-cloud-users) team. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Kubernetes (k8s)](../how-to/K8s/Terraform/tf-intro)\n",
    "We have a kubernetes cluster that we can utilize to host containers. We can provide users a private namespace to deploy to. We can also provision full kubernetes clusters for users via Rancher. Users would be administrators of their own k8s clusters but would have more freedom to customize to their needs and requirements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual Machines (VMs)\n",
    "We can provide VMs to users as needed for tasks that aren't well suited for running in a container. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFS\n",
    "We can provide general shared storage in the form of an NFS volume. Access to these systems is restricted by IP address and mounting is limited to on-premise machines. NFS mounting across wide area networks is not recommended.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Storage\n",
    "Object Storage is available and our admins can create new buckets and assign user permissions for S3 interactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Services\n",
    "Our systems will have routable IP addresses assigned and configured via DHCP. DNS records can also be added to provide full name resolution for systems provided. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}