# With a Kubernetes cluster available and HELM installed, this HELM values
# file will install JupyterHub in the Kubernetes cluster. It is also assumed
# that you have helm and kubectl installed and configured on your workstation.
# This HELM values file will require modifications which are found and 
# described by searching for 'MODS'
# Use this file as a values.yaml reference when installing the zero2jupyterhub helm chart
# following the documentation : https://z2jh.jupyter.org/en/stable/jupyterhub/installation.html#install-jupyterhub
jupyterhub:
  hub:
    config:
      JupyterHub:
      # <MODS> You will need to configure your authentication below. Below is currently
      # a template for Github auth and you need to fill out client_id, client_secret
      # and adjust the oauth_callback_url. This info can be generated within Developer settings
      # in Github via OAuth Apps using that oauth_callback_url you define.
      # Here is a reference to other auth ways:
      # https://jupyterhub.readthedocs.io/en/stable/tutorial/getting-started/authenticators-users-basics.html
      # </MODS>
        authenticator_class: github
      GitHubOAuthenticator:
        client_id: 
        client_secret: 
        oauth_callback_url: https://jupyter.k8s.ucar.edu/hub/oauth_callback
        allowed_organizations:
          - NCAR:2i2c-cloud-users
        scope:
          - read:org
      Authenticator:
      # <MODS> adjust the admin_users if needed. These use Github usernames </MODS>
        admin_users:
          - kcote-ncar # Ken Cote, Initial adminstrator
          - NicholasCote # Nicholas Cote, Initial adminstrator
          - nwehrheim # Nick Wehrheim, Community representative
          - barkathaliucar # Barkath Mir, Initial adminstrator
  proxy:
    # <MODS> Make sure you fill in a secretToken. Run: 'openssl rand -hex 32'
    # on your local workstation to get that value. </MODS>
    secretToken: ''
    service:
      type: ClusterIP
  singleuser:
    extraEnv:
      # About DASK_ prefixed variables we set:
      #
      # 1. k8s native variable expansion is applied with $(MY_ENV) syntax. The
      #    order variables are defined matters though and we are under the
      #    mercy of how KubeSpawner renders our passed dictionaries.
      #
      # 2. Dask loads local YAML config.
      #
      # 3. Dask loads environment variables prefixed DASK_.
      #    - DASK_ is stripped
      #    - Capitalization is ignored
      #    - Double underscore means a nested configuration
      #    - `ast.literal_eval` is used to parse values
      #
      # 4. dask-gateway and dask-distributed looks at its config and expands
      #    expressions in {} again, sometimes only with the environment
      #    variables as context but sometimes also with additional variables.
      #
      # References:
      # - K8s expansion:     https://kubernetes.io/docs/tasks/inject-data-application/define-interdependent-environment-variables/
      # - KubeSpawner issue: https://github.com/jupyterhub/kubespawner/issues/491
      # - Dask config:       https://docs.dask.org/en/latest/configuration.html
      # - Exploration issue: https://github.com/2i2c-org/infrastructure/issues/442
      #
      # DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE makes the default worker image
      # match the singleuser image.
      DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: "$(JUPYTER_IMAGE_SPEC)"
      # DASK_GATEWAY__CLUSTER__OPTIONS__ENVIRONMENT makes some environment
      # variables be copied over to the worker nodes from the user nodes.
      #DASK_GATEWAY__CLUSTER__OPTIONS__ENVIRONMENT: '$"SCRATCH_BUCKET": "$(SCRATCH_BUCKET)", "PANGEO_SCRATCH": "$(PANGEO_SCRATCH)"'
      # DASK_DISTRIBUTED__DASHBOARD__LINK makes the suggested link to the
      # dashboard account for the /user/<username>/<server-name> prefix in the path.
      # JUPYTERHUB_SERVICE_PREFIX has leading and trailing slashes as appropriate
      DASK_DISTRIBUTED__DASHBOARD__LINK: "$(JUPYTERHUB_SERVICE_PREFIX)proxy/{port}/status"
    storage:
      extraVolumes:
          - name: nfs-volume
            nfs:
                server: sisgs200.nwsc.ucar.edu
                path: /mnt/nwscsisds01/jupyter/shared
          - name: glade-collections
            nfs:
                server: gladedm1.ucar.edu
                path: /gpfs/fs1/collections
          - name: campaign
            nfs:
                server: gladedm1.ucar.edu
                path: /gpfs/csfs1
      extraVolumeMounts:
          - name: nfs-volume
            mountPath: /home/jovyan/shared
            readOnly: true
          - name: glade-collections
            mountPath: /glade/collections
            readOnly: true
          - name: campaign
            mountPath: /glade/campaign
            readOnly: true
    image:
      # image choice preliminary and is expected to be setup via
      # https://ncar-cisl.2i2c.cloud/services/configurator/ by the community
      #
      # pangeo/pangeo-notebook is maintained at: https://github.com/pangeo-data/pangeo-docker-images
      name: pangeo/pangeo-notebook
      tag: "2023.05.18"
    profileList:
      # NOTE: About node sharing
      #
      #       CPU/Memory requests/limits are actively considered still. This
      #       profile list is setup to involve node sharing as considered in
      #       https://github.com/2i2c-org/infrastructure/issues/2121.
      #
      #       - Memory requests are different from the description, based on:
      #         whats found to remain allocate in k8s, subtracting 1GiB
      #         overhead for misc system pods, and transitioning from GB in
      #         description to GiB in mem_guarantee.
      #       - CPU requests are lower than the description, with a factor of
      #         10%.
      #
      - display_name: "Small: up to 4 CPU / 32 GB RAM"
        description: &profile_list_description "Start a container with at least a chosen share of capacity on a node of this type"
        slug: small
        default: true
        profile_options:
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_1:
                default: true
                display_name: ~1 GB, ~0.125 CPU
                kubespawner_override:
                  mem_guarantee: 0.904G
                  cpu_guarantee: 0.013
              mem_2:
                display_name: ~2 GB, ~0.25 CPU
                kubespawner_override:
                  mem_guarantee: 1.809G
                  cpu_guarantee: 0.025
              mem_4:
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.617G
                  cpu_guarantee: 0.05
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 7.234G
                  cpu_guarantee: 0.1
              mem_16:
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 14.469G
                  cpu_guarantee: 0.2
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 28.937G
                  cpu_guarantee: 0.4
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
  #        node_selector:
  #          node.kubernetes.io/instance-type: r5.xlarge
      - display_name: "Medium: up to 16 CPU / 128 GB RAM"
        description: *profile_list_description
        slug: medium
        profile_options:
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_1:
                display_name: ~1 GB, ~0.125 CPU
                kubespawner_override:
                  mem_guarantee: 0.942G
                  cpu_guarantee: 0.013
              mem_2:
                display_name: ~2 GB, ~0.25 CPU
                kubespawner_override:
                  mem_guarantee: 1.883G
                  cpu_guarantee: 0.025
              mem_4:
                default: true
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.766G
                  cpu_guarantee: 0.05
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 7.532G
                  cpu_guarantee: 0.1
              mem_16:
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 15.064G
                  cpu_guarantee: 0.2
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 30.128G
                  cpu_guarantee: 0.4
              mem_64:
                display_name: ~64 GB, ~8.0 CPU
                kubespawner_override:
                  mem_guarantee: 60.257G
                  cpu_guarantee: 0.8
              mem_128:
                display_name: ~128 GB, ~16.0 CPU
                kubespawner_override:
                  mem_guarantee: 120.513G
                  cpu_guarantee: 1.6
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
  #        node_selector:
  #          node.kubernetes.io/instance-type: r5.4xlarge
      - display_name: "Large: up to 64 CPU / 512 GB RAM"
        description: *profile_list_description
        slug: large
        profile_options:
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_4:
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.821G
                  cpu_guarantee: 0.05
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 7.643G
                  cpu_guarantee: 0.1
              mem_16:
                default: true
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 15.285G
                  cpu_guarantee: 0.2
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 30.571G
                  cpu_guarantee: 0.4
              mem_64:
                display_name: ~64 GB, ~8.0 CPU
                kubespawner_override:
                  mem_guarantee: 61.141G
                  cpu_guarantee: 0.8
              mem_128:
                display_name: ~128 GB, ~16.0 CPU
                kubespawner_override:
                  mem_guarantee: 122.282G
                  cpu_guarantee: 1.6
              mem_256:
                display_name: ~256 GB, ~32.0 CPU
                kubespawner_override:
                  mem_guarantee: 244.565G
                  cpu_guarantee: 3.2
              mem_512:
                display_name: ~512 GB, ~64.0 CPU
                kubespawner_override:
                  mem_guarantee: 489.13G
                  cpu_guarantee: 6.4
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
  #        node_selector:
  #          node.kubernetes.io/instance-type: r5.16xlarge

      - display_name: NVIDIA Tesla T4, ~16 GB, ~4 CPUs
        slug: gpu
        description: "Start a container on a dedicated node with a GPU"
        profile_options:
          image:
            display_name: Image
            choices:
              tensorflow:
                display_name: Pangeo Tensorflow ML Notebook
                slug: "tensorflow"
                kubespawner_override:
                  image: "pangeo/ml-notebook:2023.05.18"
              pytorch:
                display_name: Pangeo PyTorch ML Notebook
                default: true
                slug: "pytorch"
                kubespawner_override:
                  image: "pangeo/pytorch-notebook:2023.05.18"
        kubespawner_override:
          mem_limit: null
          mem_guarantee: 14G
          environment:
            NVIDIA_DRIVER_CAPABILITIES: compute,utility
  ##        node_selector:
  ##          node.kubernetes.io/instance-type: g4dn.xlarge
          extra_pod_config:
            runtimeClassName: "nvidia"
          extra_resource_limits:
            nvidia.com/gpu: "1"
      - display_name: "Test NCAR Custom"
        description: "6 GB of memory; up to 4 vCPUS"
        profile_options:
          image:
            display_name: Latest
            choices:
              tensorflow:
                display_name: NCAR Latest Test
                slug: "ncar"
                kubespawner_override:
                  image: "cislcloudpilot/cisl-cloud-base:2023-09-12"
        kubespawner_override:
            mem_guarantee: 2G
            mem_limit: 6G
            cpu_guarantee: 1
            cpu_limit: 4

  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 600m
      nginx.org/client-max-body-size: "10m"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "1800"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "1800"
      nginx.ingress.kubernetes.io/rewrite-target: /
      nginx.ingress.kubernetes.io/secure-backends: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/websocket-services: proxy-public
      nginx.org/websocket-services: proxy-public
      cert-manager.io/cluster-issuer: "incommon"
    ingressClassName: nginx
    # <MODS> Adjust the hosts entries in this next session.
    hosts: [jupyter.k8s.ucar.edu]
    tls:
      - hosts:
          - jupyter.k8s.ucar.edu
        secretName: https-auto-incommon
     # </MODS>
dask-gateway:
  enabled: true # Enabling dask-gateway will install Dask Gateway as a dependency.
  # Futher Dask Gateway configuration goes here
  # See https://github.com/dask/dask-gateway/blob/master/resources/helm/dask-gateway/values.yaml
  gateway:
    backend:
      scheduler:
        extraPodConfig:
#          serviceAccountName: user-sa
#          tolerations:
#            # Let's put schedulers on notebook nodes, since they aren't ephemeral
#            # dask can recover from dead workers, but not dead schedulers
#            - key: "hub.jupyter.org/dedicated"
#              operator: "Equal"
#              value: "user"
#              effect: "NoSchedule"
#            - key: "hub.jupyter.org_dedicated"
#              operator: "Equal"
#              value: "user"
#              effect: "NoSchedule"
#          nodeSelector:
#            k8s.dask.org/node-purpose: scheduler
        cores:
          request: 0.01
          limit: 1
        memory:
          request: 128M
          limit: 1G
      worker:
        extraContainerConfig:
          securityContext:
            runAsGroup: 1000
            runAsUser: 1000
        extraPodConfig:
#          serviceAccountName: user-sa
          securityContext:
            fsGroup: 1000
#          tolerations:
#            - key: "k8s.dask.org/dedicated"
#              operator: "Equal"
#              value: "worker"
#              effect: "NoSchedule"
#            - key: "k8s.dask.org_dedicated"
#              operator: "Equal"
#              value: "worker"
#              effect: "NoSchedule"
#          nodeSelector:
#            # Dask workers get their own pre-emptible pool
#            k8s.dask.org/node-purpose: worker

    # TODO: figure out a replacement for userLimits.
    extraConfig:
      optionHandler: |
        from dask_gateway_server.options import Options, Integer, Float, String, Mapping
        import string

        # Escape a string to be dns-safe in the same way that KubeSpawner does it.
        # Reference https://github.com/jupyterhub/kubespawner/blob/616f72c4aee26c3d2127c6af6086ec50d6cda383/kubespawner/spawner.py#L1828-L1835
        # Adapted from https://github.com/minrk/escapism to avoid installing the package
        # in the dask-gateway api pod which would have been problematic.
        def escape_string_label_safe(to_escape):
            safe_chars = set(string.ascii_lowercase + string.digits)
            escape_char = "-"
            chars = []
            for c in to_escape:
                if c in safe_chars:
                    chars.append(c)
                else:
                    # escape one character
                    buf = []
                    # UTF-8 uses 1 to 4 bytes per character, depending on the Unicode symbol
                    # so we need to transform each byte to its hex value
                    for byte in c.encode("utf8"):
                        buf.append(escape_char)
                        # %X is the hex value of the byte
                        buf.append('%X' % byte)
                    escaped_hex_char = "".join(buf)
                    chars.append(escaped_hex_char)
            return u''.join(chars)

        def cluster_options(user):
            safe_username = escape_string_label_safe(user.name)
            def option_handler(options):
                if ":" not in options.image:
                    raise ValueError("When specifying an image you must also provide a tag")
                scheduler_extra_pod_annotations = {
                    "hub.jupyter.org/username": safe_username,
                    "prometheus.io/scrape": "true",
                    "prometheus.io/port": "8787",
                }
                extra_labels = {
                    "hub.jupyter.org/username": safe_username,
                }
                return {
                    "worker_cores_limit": options.worker_cores,
                    "worker_cores": options.worker_cores,
                    "worker_memory": "%fG" % options.worker_memory,
                    "image": options.image,
                    "scheduler_extra_pod_annotations": scheduler_extra_pod_annotations,
                    "scheduler_extra_pod_labels": extra_labels,
                    "worker_extra_pod_labels": extra_labels,
                    "environment": options.environment,
                }
            return Options(
                Integer("worker_cores", 4, min=1, label="Worker Cores"),
                Float("worker_memory", 8, min=1, label="Worker Memory (GiB)"),
                # The default image is set via DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE env variable
                String("image", label="Image"),
                Mapping("environment", {}, label="Environment Variables"),
                handler=option_handler,
            )
        c.Backend.cluster_options = cluster_options
      idle: |
        # timeout after 30 minutes of inactivity
        c.KubeClusterConfig.idle_timeout = 1800
    prefix: "/services/dask-gateway" # Users connect to the Gateway through the JupyterHub service.
    auth:
      type: jupyterhub # Use JupyterHub to authenticate with Dask Gateway
  traefik:
#    nodeSelector:
#      k8s.dask.org/node-purpose: core
    service:
      type: ClusterIP # Access Dask Gateway through JupyterHub. To access the Gateway from outside JupyterHub, this must be changed to a `LoadBalancer`.

# A placeholder as global values that can be referenced from the same location
# of any chart should be possible to provide, but aren't necessarily provided or
# used.
#global: {}
