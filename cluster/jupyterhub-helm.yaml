jupyterhub:
  hub:
    config:
      JupyterHub:
        authenticator_class: github
      GitHubOAuthenticator:
        client_id: 
        client_secret: 
        oauth_callback_url: https://jupyter.k8s.ucar.edu/hub/oauth_callback
        allowed_organizations:
          - NCAR:2i2c-cloud-users
        scope:
          - read:org
      Authenticator:
        admin_users:
          - kcote-ncar # Ken Cote, Initial adminstrator
          - NicholasCote # Nicholas Cote, Initial adminstrator
          - nwehrheim # Nick Wehrheim, Community representative
  proxy:
    secretToken: ''
    service:
      type: ClusterIP
  singleuser:
    storage:
      extraVolumes:
          - name: nfs-volume
            nfs:
                server: sisgs200.nwsc.ucar.edu
                path: /mnt/nwscsisds01/jupyter/shared
          - name: glade-collections
            nfs:
                server: gladedm1.ucar.edu
                path: /gpfs/fs1/collections
          - name: campaign
            nfs:
                server: gladedm1.ucar.edu
                path: /gpfs/csfs1
      extraVolumeMounts:
          - name: nfs-volume
            mountPath: /home/jovyan/shared
            readOnly: true
          - name: glade-collections
            mountPath: /glade/collections
            readOnly: true
          - name: campaign
            mountPath: /glade/campaign
            readOnly: true
    image:
      # image choice preliminary and is expected to be setup via
      # https://ncar-cisl.2i2c.cloud/services/configurator/ by the community
      #
      # pangeo/pangeo-notebook is maintained at: https://github.com/pangeo-data/pangeo-docker-images
      name: pangeo/pangeo-notebook
      tag: "2023.05.18"
    profileList:
      # NOTE: About node sharing
      #
      #       CPU/Memory requests/limits are actively considered still. This
      #       profile list is setup to involve node sharing as considered in
      #       https://github.com/2i2c-org/infrastructure/issues/2121.
      #
      #       - Memory requests are different from the description, based on:
      #         whats found to remain allocate in k8s, subtracting 1GiB
      #         overhead for misc system pods, and transitioning from GB in
      #         description to GiB in mem_guarantee.
      #       - CPU requests are lower than the description, with a factor of
      #         10%.
      #
      - display_name: "Small: up to 4 CPU / 32 GB RAM"
        description: &profile_list_description "Start a container with at least a chosen share of capacity on a node of this type"
        slug: small
        default: true
        profile_options:
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_1:
                default: true
                display_name: ~1 GB, ~0.125 CPU
                kubespawner_override:
                  mem_guarantee: 0.904G
                  cpu_guarantee: 0.013
              mem_2:
                display_name: ~2 GB, ~0.25 CPU
                kubespawner_override:
                  mem_guarantee: 1.809G
                  cpu_guarantee: 0.025
              mem_4:
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.617G
                  cpu_guarantee: 0.05
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 7.234G
                  cpu_guarantee: 0.1
              mem_16:
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 14.469G
                  cpu_guarantee: 0.2
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 28.937G
                  cpu_guarantee: 0.4
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
  #        node_selector:
  #          node.kubernetes.io/instance-type: r5.xlarge
      - display_name: "Medium: up to 16 CPU / 128 GB RAM"
        description: *profile_list_description
        slug: medium
        profile_options:
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_1:
                display_name: ~1 GB, ~0.125 CPU
                kubespawner_override:
                  mem_guarantee: 0.942G
                  cpu_guarantee: 0.013
              mem_2:
                display_name: ~2 GB, ~0.25 CPU
                kubespawner_override:
                  mem_guarantee: 1.883G
                  cpu_guarantee: 0.025
              mem_4:
                default: true
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.766G
                  cpu_guarantee: 0.05
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 7.532G
                  cpu_guarantee: 0.1
              mem_16:
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 15.064G
                  cpu_guarantee: 0.2
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 30.128G
                  cpu_guarantee: 0.4
              mem_64:
                display_name: ~64 GB, ~8.0 CPU
                kubespawner_override:
                  mem_guarantee: 60.257G
                  cpu_guarantee: 0.8
              mem_128:
                display_name: ~128 GB, ~16.0 CPU
                kubespawner_override:
                  mem_guarantee: 120.513G
                  cpu_guarantee: 1.6
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
  #        node_selector:
  #          node.kubernetes.io/instance-type: r5.4xlarge
      - display_name: "Large: up to 64 CPU / 512 GB RAM"
        description: *profile_list_description
        slug: large
        profile_options:
          requests:
            # NOTE: Node share choices are in active development, see comment
            #       next to profileList: above.
            display_name: Node share
            choices:
              mem_4:
                display_name: ~4 GB, ~0.5 CPU
                kubespawner_override:
                  mem_guarantee: 3.821G
                  cpu_guarantee: 0.05
              mem_8:
                display_name: ~8 GB, ~1.0 CPU
                kubespawner_override:
                  mem_guarantee: 7.643G
                  cpu_guarantee: 0.1
              mem_16:
                default: true
                display_name: ~16 GB, ~2.0 CPU
                kubespawner_override:
                  mem_guarantee: 15.285G
                  cpu_guarantee: 0.2
              mem_32:
                display_name: ~32 GB, ~4.0 CPU
                kubespawner_override:
                  mem_guarantee: 30.571G
                  cpu_guarantee: 0.4
              mem_64:
                display_name: ~64 GB, ~8.0 CPU
                kubespawner_override:
                  mem_guarantee: 61.141G
                  cpu_guarantee: 0.8
              mem_128:
                display_name: ~128 GB, ~16.0 CPU
                kubespawner_override:
                  mem_guarantee: 122.282G
                  cpu_guarantee: 1.6
              mem_256:
                display_name: ~256 GB, ~32.0 CPU
                kubespawner_override:
                  mem_guarantee: 244.565G
                  cpu_guarantee: 3.2
              mem_512:
                display_name: ~512 GB, ~64.0 CPU
                kubespawner_override:
                  mem_guarantee: 489.13G
                  cpu_guarantee: 6.4
        kubespawner_override:
          cpu_limit: null
          mem_limit: null
  #        node_selector:
  #          node.kubernetes.io/instance-type: r5.16xlarge

      - display_name: NVIDIA Tesla T4, ~16 GB, ~4 CPUs
        slug: gpu
        description: "Start a container on a dedicated node with a GPU"
        profile_options:
          image:
            display_name: Image
            choices:
              tensorflow:
                display_name: Pangeo Tensorflow ML Notebook
                slug: "tensorflow"
                kubespawner_override:
                  image: "pangeo/ml-notebook:2023.05.18"
              pytorch:
                display_name: Pangeo PyTorch ML Notebook
                default: true
                slug: "pytorch"
                kubespawner_override:
                  image: "pangeo/pytorch-notebook:2023.05.18"
        kubespawner_override:
          mem_limit: null
          mem_guarantee: 14G
          environment:
            NVIDIA_DRIVER_CAPABILITIES: compute,utility
  ##        node_selector:
  ##          node.kubernetes.io/instance-type: g4dn.xlarge
          extra_resource_limits:
            nvidia.com/gpu: "1"
      - display_name: "Test NCAR Custom"
        description: "6 GB of memory; up to 4 vCPUS"
        profile_options:
          image:
            display_name: Latest
            choices:
              tensorflow:
                display_name: NCAR Latest Test
                slug: "ncar"
                kubespawner_override:
                  image: "cislcloudpilot/cisl-cloud-base:v1-stable"
        kubespawner_override:
            mem_guarantee: 2G
            mem_limit: 6G
            cpu_guarantee: 1
            cpu_limit: 4

  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 600m
      nginx.org/client-max-body-size: "10m"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "1800"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "1800"
      nginx.ingress.kubernetes.io/rewrite-target: /
      nginx.ingress.kubernetes.io/secure-backends: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/websocket-services: proxy-public
      nginx.org/websocket-services: proxy-public
      cert-manager.io/cluster-issuer: "incommon"
    ingressClassName: nginx
    hosts: [jupyter.k8s.ucar.edu]
    tls:
      - hosts:
          - jupyter.k8s.ucar.edu
        secretName: https-auto-incommon
dask-gateway: