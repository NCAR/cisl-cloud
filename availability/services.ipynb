{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JupyterHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Documentation](../how-to/k8sJH/k8sJH-intro)\n",
    "***STATUS:*** *In Testing*\n",
    "\n",
    "**URL :** [https://jupyter.k8s.ucar.edu/](https://jupyter.k8s.ucar.edu/)\n",
    "\n",
    "\n",
    "An on-premise JupyterHub is up and running, but is still in development. Authentication is handled via a GitHub team in the NCAR organization. The user space that is spun up has access to a shared NFS volume that is read-only as well as GLADE collections and campaign directories also mounted as read-only. The user notebooks that can be deployed are custom container images that the CCPP team maintains. Documentation on how this was setup and deployed can be found on the [how-to](../how-to/k8sJH/customize/customize-docker) page in this documentation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Documentation](../how-to/binderhub/binderhub)\n",
    "***STATUS:*** *In Testing*\n",
    "\n",
    "**URL :** [https://binder.k8s.ucar.edu/](https://binder.k8s.ucar.edu/)\n",
    "\n",
    "Binder is hosted On-Premise and has its own dedicated JupyterHub to deploy containerized code repositories. Authentication is handled via the same GitHub team in the NCAR organization as the JupyterHub. Binder also has access to GLADE campaign and collections as well as internal resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS instance supported by 2i2c](../how-to/2i2cJH/2i2cJH-intro)\n",
    "***STATUS:*** *In Testing*\n",
    "\n",
    "**URL :** [https://ncar-cisl.2i2c.cloud/](https://ncar-cisl.2i2c.cloud/)\n",
    "\n",
    "The JupyterHub instance setup and managed by 2i2c that runs on AWS is up and ready to use. Access to this JupyterHub is controlled via a GitHub team, specifically the NCAR organizations [2i2c-cloud-users](https://github.com/orgs/NCAR/teams/2i2c-cloud-users) team. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Kubernetes (k8s)](../how-to/K8s/k8s-intro)\n",
    "We have a kubernetes cluster that we can utilize to host containers. We can provide users a private namespace to deploy to. We can also provision full kubernetes clusters for users via Rancher. Users would be administrators of their own k8s clusters but would have more freedom to customize to their needs and requirements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Rancher](../how-to/K8s/Rancher/rancher-intro)\n",
    "***STATUS:*** *In Testing*\n",
    "\n",
    "**URL :** [https://rancher.k8s.ucar.edu/](https://rancher.k8s.ucar.edu/)\n",
    "\n",
    "We have an instance of Rancher running that provides a user interface for interacting with k8s clusters. Users can request access to the rancher instance, login, and get a kubeconfig to access and deploy k8s resources. User permissions are controlled in a way that limits what they can view and deploy to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Harbor](../how-to/K8s/harbor/harbor-intro)\n",
    "***STATUS:*** *In Testing*\n",
    "\n",
    "**URL :** [https://hub.k8s.ucar.edu/](https://hub.k8s.ucar.edu/)\n",
    "\n",
    "We utilize [Harbor](https://goharbor.io/) to provide a container registry based on open source software that is closer to the infrastructure running containers. A local registry allows us to utilize network infrastructure and available bandwidth between hardware for an increase in speed when pushing and pulling images locally. Harbor also includes an image scanner that will provide reports on any vulnerabilities that an image contains so we can address security concerns with images directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Argo CD](../how-to/K8s/Hosting/deploy)\n",
    "***STATUS:*** *In Testing*\n",
    "\n",
    "We have an instance of [Argo CD](https://argo-cd.readthedocs.io/en/stable/) installed to help us handle Continuous Delivery (CD). What this ultimately means is if your applications Git repo is setup in Argo CD it can be automatically configured to deploy any changes made to that repository without any intervention by the user or admins. This allows users to deploy their applications automatically to k8s without having to worry about interacting directly with Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Virtual Machines (VMs)](../how-to/vm/vm-intro)\n",
    "We can provide VMs to users as needed for tasks that aren't well suited for running in a container. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistent Volumes\n",
    "[Rook](https://rook.io/docs/rook/v1.11/Getting-Started/intro/) is used to provide storage orchestration to k8s workloads. Rook utilizes Ceph as a distributed storage system to provide persistent file, block, and object storage capabilities to the k8s cluster and the underlying objects hosted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLADE Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read only access to data stored on [GLADE](https://arc.ucar.edu/knowledge_base/68878466) is provided via NFS to the K8s nodes which is then exposed to objects in the cluster via Rook. GLADE is managed by the by the [Advanced Research Computing](https://arc.ucar.edu/) division of NSF NCAR | CISL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Storage\n",
    "Object Storage is available via [Stratus](../how-to/Stratus/stratus-intro) and our admins can create new buckets and assign user permissions for S3 interactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExternalDNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system has [ExternalDNS](https://bitnami.com/stack/external-dns/helm) configured to provide DNS records for full name resolution and useable URLs for hosted systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cert-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure the URLs exposed are secure we implemented [cert-manager](https://cert-manager.io/) to assign valid certificates to applications and perform lifecycle management on the issued certificates. This ensures all services are accessible only via HTTPS with valid certificates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nginx Ingress Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kubernetes cluster exposes applications to the a network by utilizing the [Nginx Ingress Controller](https://docs.nginx.com/nginx-ingress-controller/). By combining the Ingress with ExternalDNS and cert-manager routable HTTPS addresses can be tied directly to deployed applications allowing the sharing of content to the greater community. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
